{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "def RMSE(x_pred,x_true):\n",
    "   return np.sqrt(np.mean((x_true - x_pred)**2))\n",
    "def confusion_matrix(y_pred, y_true):\n",
    "    classes = np.unique(y_true)\n",
    "    n_classes = len(classes)\n",
    "    conf_matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        true_class = y_true[i]\n",
    "        pred_class = y_pred[i]\n",
    "        conf_matrix[true_class][pred_class] += 1\n",
    "        \n",
    "    return conf_matrix\n",
    "def accuracy(y_pred, y_true):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\"\"\"\"Epss : Large number -> abnormal\n",
    "Lvdd: Large hearts tend to be sick hearts\n",
    "Wall_Motion_index: should be around 12-13 \"\"\"\n",
    "\n",
    "col_names = ['Months_Survived','Still_Alive','Age_o_HA',\n",
    "'Effusion','Fractional_Short','Epss',\n",
    "'Lvdd','Wall_MotionSc','Wallmotion_index','Mult',\n",
    "'Name','Group','Alive_at_1']\n",
    "p = pd.read_csv('/home/dimitriskana/workspace/Echocardiogram/echocardiogram.data',  on_bad_lines= 'skip', names = col_names)\n",
    "Data = p.drop(['Wall_MotionSc','Name','Group','Mult'],axis=1)\n",
    "Data[Data =='?' ] = np.nan\n",
    "Data = Data.astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to clear the data. \n",
    "The data contains lot of ? values that I translated to nans for better use. \n",
    "I cleaned the data using the Still_Alive and Months_Survived columns,\n",
    "while using hints from the researchers,in order our target to be nan-free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id = Data[(Data['Months_Survived']<12) & (Data['Alive_at_1']!=1)]\n",
    "#clear the nans for patients that survived less than a year, \n",
    "# in the target column I assign the still_alive value\n",
    "Data.loc[id.index,'Alive_at_1'] = Data.iloc[id.index]['Still_Alive']\n",
    "Data.loc[Data['Months_Survived']>12,'Alive_at_1'] = 1\n",
    "#check if patients that are still alive after 12 months,\n",
    "#  are labeled as alive\n",
    "Data.loc[(Data['Months_Survived']>12)&Data['Still_Alive']==1,'Alive_at_1'] = 1\n",
    "# we create a dataframe without patients that are alive less than a year\n",
    "df = Data[~((Data['Months_Survived']<12) & (Data['Alive_at_1']==1))].reset_index().drop('index',axis =1)\n",
    "#we also want to get rid some more nans for our target,\n",
    "#  we assign the value of  the Still_Alive column to our target   \n",
    "i = df[df['Alive_at_1'].isna()].index\n",
    "df.loc[i,'Alive_at_1'] = df.loc[i,'Still_Alive']\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still we have missing values in crucial attributes of a patient. \n",
    "We will try to solve that using a variety of techniques such as : \n",
    "1. dropping all the rows that contain nans\n",
    "2. calculate the values using methods I found in a paper and comparing it to an open-source library\n",
    "3. using regression to calculate the missing values\n",
    "After that we will see what worked better and decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Months_Survived     1\n",
       "Still_Alive         0\n",
       "Age_o_HA            3\n",
       "Effusion            0\n",
       "Fractional_Short    3\n",
       "Epss                9\n",
       "Lvdd                4\n",
       "Wallmotion_index    0\n",
       "Alive_at_1          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Map of the missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping the still alive as it is incoporated to the alive at 1 and the months_survived\n",
    "# we use standardization for the X data only for the non categorical values \n",
    "Drop_Data = df.dropna().drop('Still_Alive',axis=1)\n",
    "X = (Drop_Data.copy()*Drop_Data.mean()/Drop_Data.std()).to_numpy()\n",
    "#target\n",
    "y =Drop_Data['Alive_at_1'].to_numpy().astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tree import DecisionTree as DT \n",
    "from forrest import R_Forrest as R_F\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=1234)\n",
    "D = DT(min_samples_split=2, max_depth=200)\n",
    "D.fit(X_train,y_train)\n",
    "D.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  0]\n",
      " [ 3 22]]\n",
      "0.32732683535398854\n"
     ]
    }
   ],
   "source": [
    "CM = confusion_matrix(D.predict(X_test),y_test)\n",
    "print(CM)\n",
    "F = RMSE(D.predict(X_test),y_test)\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8928571428571429"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(D.predict(X_test),y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conf_matrix: [[TN,FP\n",
    "               FN,TP]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "Forest = R_F(min_samples_split=2, max_depth=200,n_trees=2)\n",
    "Forest.fit(X_train,y_train)\n",
    "XX = Forest.predict(X_test)\n",
    "print(RMSE(XX,y_test))\n",
    "print(accuracy(XX,y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we replace the nans with their respective median  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mead_Data = df.drop('Still_Alive',axis=1)\n",
    "Mead_Data.fillna(Mead_Data.median(), inplace=True)\n",
    "Xm = (Mead_Data*Mead_Data.mean()/Mead_Data.std()).to_numpy()\n",
    "#target\n",
    "ym =Mead_Data['Alive_at_1'].to_numpy().astype('int64')\n",
    "\n",
    "X_trainm, X_testm, y_trainm, y_testm = train_test_split(X, y,\n",
    " test_size = 0.33, random_state=1234)\n",
    "D.fit(X_trainm,y_trainm)\n",
    "D.predict(X_testm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  0]\n",
      " [ 0 25]]\n",
      "The RMSE is :  0.0000\n",
      "The accuracy of the model is : 1.0000\n"
     ]
    }
   ],
   "source": [
    "CMm = confusion_matrix(D.predict(X_testm),y_testm)\n",
    "print(CMm)\n",
    "Fm = RMSE(D.predict(X_testm),y_testm)\n",
    "print(f'The RMSE is :  {Fm :.4f}')\n",
    "print(f'The accuracy of the model is : {accuracy(D.predict(X_testm),y_testm) :.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE is :  0.2673\n",
      "The accuracy of the model is : 0.9286\n"
     ]
    }
   ],
   "source": [
    "Forest.fit(X_trainm,y_trainm)\n",
    "XXm = Forest.predict(X_testm)\n",
    "#print(f'The confusion matrix : {confusion_matrix(XXm,y_testm) :.4f}')\n",
    "print(f'The RMSE is :  {RMSE(XXm,y_testm) :.4f}')\n",
    "print(f'The accuracy of the model is : {accuracy(XXm,y_testm) :.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Max Singular Value of X_init = 674.373236\n",
      "[SoftImpute] Iter 1: observed MAE=0.553216 rank=3\n",
      "[SoftImpute] Iter 2: observed MAE=0.549412 rank=3\n",
      "[SoftImpute] Iter 3: observed MAE=0.550459 rank=3\n",
      "[SoftImpute] Iter 4: observed MAE=0.551597 rank=3\n",
      "[SoftImpute] Iter 5: observed MAE=0.552465 rank=3\n",
      "[SoftImpute] Iter 6: observed MAE=0.553052 rank=3\n",
      "[SoftImpute] Iter 7: observed MAE=0.553442 rank=3\n",
      "[SoftImpute] Iter 8: observed MAE=0.553706 rank=3\n",
      "[SoftImpute] Iter 9: observed MAE=0.553875 rank=3\n",
      "[SoftImpute] Iter 10: observed MAE=0.553940 rank=3\n",
      "[SoftImpute] Iter 11: observed MAE=0.553991 rank=3\n",
      "[SoftImpute] Iter 12: observed MAE=0.553986 rank=3\n",
      "[SoftImpute] Iter 13: observed MAE=0.553942 rank=3\n",
      "[SoftImpute] Iter 14: observed MAE=0.553876 rank=3\n",
      "[SoftImpute] Iter 15: observed MAE=0.553786 rank=3\n",
      "[SoftImpute] Iter 16: observed MAE=0.553676 rank=3\n",
      "[SoftImpute] Iter 17: observed MAE=0.553555 rank=3\n",
      "[SoftImpute] Iter 18: observed MAE=0.553430 rank=3\n",
      "[SoftImpute] Iter 19: observed MAE=0.553297 rank=3\n",
      "[SoftImpute] Iter 20: observed MAE=0.553161 rank=3\n",
      "[SoftImpute] Iter 21: observed MAE=0.553034 rank=3\n",
      "[SoftImpute] Iter 22: observed MAE=0.552904 rank=3\n",
      "[SoftImpute] Iter 23: observed MAE=0.552772 rank=3\n",
      "[SoftImpute] Iter 24: observed MAE=0.552645 rank=3\n",
      "[SoftImpute] Iter 25: observed MAE=0.552520 rank=3\n",
      "[SoftImpute] Iter 26: observed MAE=0.552405 rank=3\n",
      "[SoftImpute] Iter 27: observed MAE=0.552291 rank=3\n",
      "[SoftImpute] Iter 28: observed MAE=0.552179 rank=3\n",
      "[SoftImpute] Iter 29: observed MAE=0.552068 rank=3\n",
      "[SoftImpute] Iter 30: observed MAE=0.551959 rank=3\n",
      "[SoftImpute] Iter 31: observed MAE=0.551853 rank=3\n",
      "[SoftImpute] Iter 32: observed MAE=0.551750 rank=3\n",
      "[SoftImpute] Iter 33: observed MAE=0.551649 rank=3\n",
      "[SoftImpute] Iter 34: observed MAE=0.551550 rank=3\n",
      "[SoftImpute] Iter 35: observed MAE=0.551455 rank=3\n",
      "[SoftImpute] Iter 36: observed MAE=0.551362 rank=3\n",
      "[SoftImpute] Iter 37: observed MAE=0.551272 rank=3\n",
      "[SoftImpute] Iter 38: observed MAE=0.551185 rank=3\n",
      "[SoftImpute] Iter 39: observed MAE=0.551101 rank=3\n",
      "[SoftImpute] Iter 40: observed MAE=0.551020 rank=3\n",
      "[SoftImpute] Iter 41: observed MAE=0.550941 rank=3\n",
      "[SoftImpute] Iter 42: observed MAE=0.550866 rank=3\n",
      "[SoftImpute] Iter 43: observed MAE=0.550793 rank=3\n",
      "[SoftImpute] Iter 44: observed MAE=0.550723 rank=3\n",
      "[SoftImpute] Iter 45: observed MAE=0.550655 rank=3\n",
      "[SoftImpute] Iter 46: observed MAE=0.550590 rank=3\n",
      "[SoftImpute] Iter 47: observed MAE=0.550528 rank=3\n",
      "[SoftImpute] Iter 48: observed MAE=0.550468 rank=3\n",
      "[SoftImpute] Iter 49: observed MAE=0.550410 rank=3\n",
      "[SoftImpute] Iter 50: observed MAE=0.550355 rank=3\n",
      "[SoftImpute] Iter 51: observed MAE=0.550302 rank=3\n",
      "[SoftImpute] Iter 52: observed MAE=0.550251 rank=3\n",
      "[SoftImpute] Iter 53: observed MAE=0.550202 rank=3\n",
      "[SoftImpute] Iter 54: observed MAE=0.550155 rank=3\n",
      "[SoftImpute] Iter 55: observed MAE=0.550110 rank=3\n",
      "[SoftImpute] Iter 56: observed MAE=0.550067 rank=3\n",
      "[SoftImpute] Iter 57: observed MAE=0.550026 rank=3\n",
      "[SoftImpute] Iter 58: observed MAE=0.549987 rank=3\n",
      "[SoftImpute] Iter 59: observed MAE=0.549949 rank=3\n",
      "[SoftImpute] Iter 60: observed MAE=0.549913 rank=3\n",
      "[SoftImpute] Iter 61: observed MAE=0.549878 rank=3\n",
      "[SoftImpute] Iter 62: observed MAE=0.549845 rank=3\n",
      "[SoftImpute] Iter 63: observed MAE=0.549814 rank=3\n",
      "[SoftImpute] Iter 64: observed MAE=0.549783 rank=3\n",
      "[SoftImpute] Iter 65: observed MAE=0.549754 rank=3\n",
      "[SoftImpute] Iter 66: observed MAE=0.549727 rank=3\n",
      "[SoftImpute] Iter 67: observed MAE=0.549700 rank=3\n",
      "[SoftImpute] Iter 68: observed MAE=0.549675 rank=3\n",
      "[SoftImpute] Iter 69: observed MAE=0.549651 rank=3\n",
      "[SoftImpute] Iter 70: observed MAE=0.549628 rank=3\n",
      "[SoftImpute] Iter 71: observed MAE=0.549606 rank=3\n",
      "[SoftImpute] Iter 72: observed MAE=0.549585 rank=3\n",
      "[SoftImpute] Iter 73: observed MAE=0.549565 rank=3\n",
      "[SoftImpute] Iter 74: observed MAE=0.549546 rank=3\n",
      "[SoftImpute] Iter 75: observed MAE=0.549527 rank=3\n",
      "[SoftImpute] Iter 76: observed MAE=0.549510 rank=3\n",
      "[SoftImpute] Iter 77: observed MAE=0.549493 rank=3\n",
      "[SoftImpute] Iter 78: observed MAE=0.549477 rank=3\n",
      "[SoftImpute] Iter 79: observed MAE=0.549462 rank=3\n",
      "[SoftImpute] Iter 80: observed MAE=0.549447 rank=3\n",
      "[SoftImpute] Stopped after iteration 80 for lambda=13.487465\n",
      "The confusion matrix is: [[ 2  0]\n",
      " [ 0 31]]\n",
      "Imputing row 1/97 with 0 missing, elapsed time: 0.003\n",
      "The confusion matrix is: [[ 2  0]\n",
      " [ 1 30]]\n",
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.3.0                                    \n",
      "===============================================================================\n",
      "(CVXPY) Feb 20 12:19:06 PM: Your problem has 873 variables, 1 constraints, and 0 parameters.\n",
      "(CVXPY) Feb 20 12:19:06 PM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Feb 20 12:19:06 PM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Feb 20 12:19:06 PM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Feb 20 12:19:06 PM: Compiling problem (target solver=CVXOPT).\n",
      "(CVXPY) Feb 20 12:19:06 PM: Reduction chain: Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> CVXOPT\n",
      "(CVXPY) Feb 20 12:19:06 PM: Applying reduction Dcp2Cone\n",
      "(CVXPY) Feb 20 12:19:06 PM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Feb 20 12:19:06 PM: Applying reduction ConeMatrixStuffing\n",
      "(CVXPY) Feb 20 12:19:06 PM: Applying reduction CVXOPT\n",
      "(CVXPY) Feb 20 12:19:06 PM: Finished problem compilation (took 1.138e-01 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Feb 20 12:19:06 PM: Invoking solver CVXOPT  to obtain a solution.\n",
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  0.0000e+00 -8.7300e-02  9e+05  2e+01  6e+00  1e+00\n",
      " 1:  1.0131e+03  1.0563e+03  1e+05  4e+00  1e+00  4e+01\n",
      " 2:  9.2308e+02  9.4135e+02  3e+04  1e+00  4e-01  2e+01\n",
      " 3:  8.7384e+02  8.7684e+02  5e+03  2e-01  6e-02  3e+00\n",
      " 4:  8.7604e+02  8.7704e+02  1e+03  6e-02  2e-02  1e+00\n",
      " 5:  8.7864e+02  8.7892e+02  4e+02  1e-02  5e-03  3e-01\n",
      " 6:  8.7847e+02  8.7852e+02  8e+01  3e-03  9e-04  5e-02\n",
      " 7:  8.7837e+02  8.7837e+02  2e+00  8e-05  3e-05  7e-04\n",
      " 8:  8.7841e+02  8.7841e+02  1e-01  1e-05  4e-06  7e-06\n",
      " 9:  8.7843e+02  8.7843e+02  7e-03  2e-06  6e-07  6e-06\n",
      "10:  8.7843e+02  8.7843e+02  1e-03  4e-07  1e-07  1e-06\n",
      "11:  8.7843e+02  8.7843e+02  3e-04  8e-08  3e-08  2e-07\n",
      "Optimal solution found.\n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Feb 20 12:21:52 PM: Problem status: optimal\n",
      "(CVXPY) Feb 20 12:21:52 PM: Optimal value: 8.784e+02\n",
      "(CVXPY) Feb 20 12:21:52 PM: Compilation took 1.138e-01 seconds\n",
      "(CVXPY) Feb 20 12:21:52 PM: Solver (including time spent in interface) took 1.657e+02 seconds\n",
      "The confusion matrix is: [[ 2  0]\n",
      " [ 0 31]]\n"
     ]
    }
   ],
   "source": [
    "from fancyimpute import KNN, NuclearNormMinimization, SoftImpute\n",
    "Imputers = [SoftImpute(),KNN(),NuclearNormMinimization()]\n",
    "rm = []\n",
    "accs = [ ]\n",
    "for item in Imputers : \n",
    "    new_df = pd.DataFrame()\n",
    "    new_df = pd.DataFrame(item.fit_transform(df), columns = df.columns)\n",
    "    \n",
    "    new_df = new_df.drop('Still_Alive',axis=1)\n",
    "    \n",
    "    #norm\n",
    "    X = (new_df*new_df.mean()/new_df.std()).to_numpy()\n",
    "    #target\n",
    "    y =new_df['Alive_at_1'].to_numpy().astype('int64')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=1234)\n",
    "    D.fit(X_train,y_train)\n",
    "    Pr = D.predict(X_test)\n",
    "    rm.append(RMSE(Pr,y_test))\n",
    "    accs.append(accuracy(Pr,y_test))\n",
    "    print(f'The confusion matrix is: {confusion_matrix(Pr,y_test)}')\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.9696969696969697, 1.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, We could conclude that if we repalce nans with the median works as fine as more sophisticated imputation techniques we used with fancyimpute.\n",
    "The decision tree seems to be working very well for this categorization of the dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "683ec6e70c363dd226836d2b94f38b30e3e384eb8fd3dd5ff5ad30859dec7205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
